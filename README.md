The goal of this Project was to predict whether or not a patient would be readmitted based off of the dataset. Firstly, I started with EDA - glancing over the data. Next, I made a few scatter plots, graphing time_in_hospital against num_lab_proecedures and num_medications, with the color='readmitted' to show which cases the patient was readmitted. After a few other seaborn and plotly graphs, I used plotly's 3d scatter plot function to plot time_in_hospital, num_lab_procedures, num_medications, with the symbol (shape) = diabetes, and the color='readmitted'. The results showed a slight positive correlation, with more time in hospital, number of lab procedures increasing, and number of medications increasing. Similarly, a 3D surface plot showed a high correlatio between an increase in the number of medications and time in hospital. Nextly, I imported a lot of scikit-learn modules for preprocessing, regression models, and classification models. I then defined the y_reg (y regression variable, 'time_in_hospital') and y_clf (y classification variable, 'readmitted'). I then sorted the data into numerical_cols_reg and categorical_cols_reg and numerical_cols_clf and categorical_cols_clf, then train_test_split X_reg, y_reg, and another train-test_split for X_clf, y_clf. Nextly, to handle Outliers, I used the EllipticEnvelope function from sklearn.covariance, and filtered outliers from the X_train_reg and y_train_reg data, then the X_train_clf and y_train_clf data. Ialso used SMOTE (Synthetic Minority Over-sampling Technique) to handle the imbalance between the number of readmitted and non-readmitted patients. Next, I used a majority class predictor, which always predicts 'readmitted', giving a baseline accuracy of 0.5436. I then created a regressio npipeline, and a param_grid to test different parameters, yielding an average mean Squared Error of 6.518 for Linear Regression. For classification, I tested logistic regression, decision_trees, random_forests, k_neighbors, gradient_boosting, and xgboost, with many different parameters via the RandomizedSearchCV() function. To evaluate the accuracy, I used a for loop, random searching each model and their respective parameter grids, using the classification_report() function from scikit-learn, and graphing the roc_auc_score with a roc_curve; I also graphed confusion matrix to show True Positives, False Positives, True Negatives, and False Negatives. The model with the highest ROC Curve (the higher the ROC curve, the better performing classification model it is (ability to distinguish between postiive and negative cases) was the Random Forest Classifier Model, which had a precision score of 57%, a recall score of 58%, and an f1-score of 57%. Overall, the model managed to increase accuracy by 3-4%, which while small, can still have some impact on a hospital's ability to utilize resources effiicently and prepare for readmitted patients. 
